
from Model import *
from MusicData import MusicDataset
import os
from music21 import *

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

SONG_LENGTH = 200

def to_one_hot(values, num_classes):
    return np.eye(num_classes, dtype='float')[values]

def getDurationFloat(durationString):
    if "/" in durationString:
        values = durationString.split("/")
        return float(float(values[0]) / float(values[1]))
    return float(durationString)

def create_midi(prediction_output, trainset):
    offset = 0
    output_notes = []
    # create note and chord objects based on the values generated by the model
    for note_full_representation in prediction_output:
        pattern = note_full_representation.split(",")[0]
        duration = note_full_representation.split(",")[1]
        print("Duration: " + duration)
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.quarterLength = getDurationFloat(duration)
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.quarterLength = getDurationFloat(duration)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a rest
        elif('rest' in pattern):
            new_rest = note.Rest(pattern)
            new_rest.quarterLength = getDurationFloat(duration)
            new_rest.offset = offset
            new_rest.storedInstrument = instrument.Piano() #???
            output_notes.append(new_rest)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.quarterLength = getDurationFloat(duration)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)

    midi_stream.write('midi', fp=generate_save_file)

def generate():
    # Initialize Network
    trainset = MusicDataset(midi_file_dir, sequence_length, notes_save_file,
                            prepared_input_save_file, prepared_output_save_file)
    generateLoader = torch.utils.data.DataLoader(trainset, batch_size=1,
                                            shuffle=True, num_workers=0)

    myNet = Net(trainset.num_unique_notes)
    print(myNet)

    # Load Weights
    if os.path.isfile(model_save_file):
        checkpoint = torch.load(model_save_file)
        myNet.load_state_dict(checkpoint[CHECK_MODEL_STATE])
        # optimizer.load_state_dict(checkpoint[CHECK_OPTIMIZER_STATE])
        # epoch = checkpoint[CHECK_EPOCH]
        # loss = checkpoint['loss']
    else:
        print("No saved weights found at: " + model_save_file)
        assert(True == False)

    torch.set_num_threads(numThreads)

    myNet.eval()

    with torch.no_grad():
        # Only get 1 random sequence to start things off
        for data in generateLoader:
            initial_inputs, labels = data
            break
        
        pitchnames = sorted(set(item for item in trainset.notes))
        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))

        final_outputs = []
        for i in range(SONG_LENGTH):
            output, (hn, cn) = myNet(initial_inputs, 1)
            _, predicted = torch.max(output.data, 1)
            # print(predicted.shape)
            # print(predicted.numpy()[0])
            # print(initial_inputs.shape)
            # print(initial_inputs)
            
            int_out = predicted.numpy()[0]
            final_outputs.append(int_to_note[int_out])
            for j in range(1, len(initial_inputs[0])):
                initial_inputs[0][j-1] = initial_inputs[0][j]
            initial_inputs[0][sequence_length-1] = torch.from_numpy(to_one_hot(int_out, trainset.num_unique_notes))

    create_midi(final_outputs, trainset)

#generate()
